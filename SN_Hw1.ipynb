{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33cea5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files loaded successfully.\n",
      "Training data shape: (21090, 2)\n",
      "Test data shape: (8000, 3)\n",
      "Graph built with 10980 nodes and 21090 edges.\n",
      "Average degree: 3.841530054644809\n",
      "Clustering coefficient: 0.13466795034502793\n",
      "Generated 1000 negative samples...\n",
      "Generated 2000 negative samples...\n",
      "Generated 3000 negative samples...\n",
      "Generated 4000 negative samples...\n",
      "Generated 5000 negative samples...\n",
      "Generated 6000 negative samples...\n",
      "Generated 7000 negative samples...\n",
      "Generated 8000 negative samples...\n",
      "Generated 9000 negative samples...\n",
      "Generated 10000 negative samples...\n",
      "Generated 11000 negative samples...\n",
      "Generated 12000 negative samples...\n",
      "Generated 13000 negative samples...\n",
      "Generated 14000 negative samples...\n",
      "Generated 15000 negative samples...\n",
      "Generated 16000 negative samples...\n",
      "Generated 17000 negative samples...\n",
      "Generated 18000 negative samples...\n",
      "Generated 19000 negative samples...\n",
      "Generated 20000 negative samples...\n",
      "Generated 21000 negative samples...\n",
      "Generated 22000 negative samples...\n",
      "Generated 23000 negative samples...\n",
      "Generated 24000 negative samples...\n",
      "Generated 25000 negative samples...\n",
      "Generated 26000 negative samples...\n",
      "Generated 27000 negative samples...\n",
      "Generated 28000 negative samples...\n",
      "Generated 29000 negative samples...\n",
      "Generated 30000 negative samples...\n",
      "Generated 31000 negative samples...\n",
      "Generated 32000 negative samples...\n",
      "Generated 33000 negative samples...\n",
      "Generated 34000 negative samples...\n",
      "Generated 35000 negative samples...\n",
      "Generated 36000 negative samples...\n",
      "Generated 37000 negative samples...\n",
      "Generated 38000 negative samples...\n",
      "Generated 39000 negative samples...\n",
      "Generated 40000 negative samples...\n",
      "Generated 41000 negative samples...\n",
      "Generated 42000 negative samples...\n",
      "Generated 42180 negative samples.\n",
      "Calculating features for positive training examples...\n",
      "Calculating features for negative training examples...\n",
      "Training features prepared.\n",
      "    jaccard  adamic_adar  common_neighbors  degree_u  degree_v\n",
      "0  0.250000     1.442695                 1         2         3\n",
      "1  0.266667     5.807699                24        61        53\n",
      "2  0.166667     0.283578                 1         4         3\n",
      "3  0.000000     0.000000                 0         7         4\n",
      "4  0.297619     6.287469                25        61        48\n",
      "Training features shape: (63270, 5)\n",
      "Training labels length: 63270\n",
      "Splitting data into 70% training and 30% validation...\n",
      "Training split shape: (44289, 5)\n",
      "Validation split shape: (18981, 5)\n",
      "Training XGBClassifier on 70% training split...\n",
      "Calculating training metrics...\n",
      "Training accuracy: 0.8986\n",
      "Training F1-score: 0.8222\n",
      "Training AUC: 0.8979\n",
      "Calculating validation metrics...\n",
      "Validation accuracy: 0.8956\n",
      "Validation F1-score: 0.8161\n",
      "Validation AUC: 0.8161\n",
      "Feature importances: {'jaccard': np.float32(0.017627876), 'adamic_adar': np.float32(0.0017213889), 'common_neighbors': np.float32(0.97983605), 'degree_u': np.float32(0.00057929085), 'degree_v': np.float32(0.00023537192)}\n",
      "Performing 5-fold cross-validation on full training data...\n",
      "Cross-validation accuracy: 0.8974 ± 0.0026\n",
      "Cross-validation F1-score: 0.8195 ± 0.0055\n",
      "Training ensemble (XGBoost + RandomForest) on full training data...\n",
      "Ensemble classifiers trained.\n",
      "Calculating features for test examples...\n",
      "Test features prepared.\n",
      "Test features shape: (8000, 5)\n",
      "Predicting on test data with ensemble...\n",
      "Formatting submission file...\n",
      "Submission file 'submission.csv' created successfully.\n",
      "   predict_nodepair_id  ans\n",
      "0                    0    1\n",
      "1                    1    0\n",
      "2                    2    1\n",
      "3                    3    0\n",
      "4                    4    0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "import random\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- 1. Load Data ---\n",
    "try:\n",
    "    train_df = pd.read_csv(r'C:\\Users\\Dspike\\Documents\\NTUST\\2ndsemester\\Social Media Analytics\\train.csv')\n",
    "    test_df = pd.read_csv(r'C:\\Users\\Dspike\\Documents\\NTUST\\2ndsemester\\Social Media Analytics\\test.csv')\n",
    "    print(\"Files loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: training.csv or test.csv not found. Creating dummy data for demonstration...\")\n",
    "    train_df = pd.DataFrame({\n",
    "        'node1': [0, 0, 1, 1, 2, 3, 4, 6],\n",
    "        'node2': [1, 2, 3, 4, 5, 4, 5, 7]\n",
    "    })\n",
    "    test_df = pd.DataFrame({\n",
    "        'node1': [0, 1, 2, 5, 6, 7, 0, 3],\n",
    "        'node2': [3, 5, 4, 7, 0, 1, 6, 6]\n",
    "    })\n",
    "    test_df['predict_nodepair_id'] = range(len(test_df))\n",
    "\n",
    "print(f\"Training data shape: {train_df.shape}\")\n",
    "print(f\"Test data shape: {test_df.shape}\")\n",
    "\n",
    "# --- 2. Build Graph ---\n",
    "G = nx.Graph()\n",
    "G.add_edges_from(train_df[['node1', 'node2']].values)\n",
    "print(f\"Graph built with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges.\")\n",
    "\n",
    "# Print graph statistics\n",
    "print(\"Average degree:\", np.mean([d for _, d in G.degree()]))\n",
    "print(\"Clustering coefficient:\", nx.average_clustering(G))\n",
    "\n",
    "# List of all nodes in the training graph\n",
    "nodes_list = list(G.nodes())\n",
    "num_nodes = len(nodes_list)\n",
    "\n",
    "# --- 3. Generate Negative Training Samples ---\n",
    "num_positive_samples = len(train_df)\n",
    "num_negative_samples = 2 * num_positive_samples  # 2:1 negative:positive ratio\n",
    "\n",
    "negative_samples = []\n",
    "existing_edges = set(G.edges()) | set([(v, u) for u, v in G.edges()])  # Include both directions\n",
    "\n",
    "while len(negative_samples) < num_negative_samples:\n",
    "    u, v = random.sample(nodes_list, 2)\n",
    "    if (u, v) not in existing_edges and (v, u) not in existing_edges:\n",
    "        negative_samples.append((u, v))\n",
    "    if len(negative_samples) % 1000 == 0:\n",
    "        print(f\"Generated {len(negative_samples)} negative samples...\")\n",
    "    if len(negative_samples) >= num_nodes * (num_nodes - 1) // 2 - G.number_of_edges():\n",
    "        print(\"Warning: Approaching max possible non-edges. Stopping negative sampling.\")\n",
    "        break\n",
    "\n",
    "print(f\"Generated {len(negative_samples)} negative samples.\")\n",
    "\n",
    "# --- 4. Feature Engineering Function ---\n",
    "def calculate_features(u, v, graph):\n",
    "    features = {}\n",
    "    try:\n",
    "        # Jaccard Coefficient\n",
    "        jaccard_scores = list(nx.jaccard_coefficient(graph, [(u, v)]))\n",
    "        features['jaccard'] = jaccard_scores[0][2] if jaccard_scores else 0\n",
    "\n",
    "        # Adamic-Adar Index\n",
    "        adamic_adar_scores = list(nx.adamic_adar_index(graph, [(u, v)]))\n",
    "        features['adamic_adar'] = adamic_adar_scores[0][2] if adamic_adar_scores else 0\n",
    "        '''\n",
    "        # Preferential Attachment\n",
    "        preferential_scores = list(nx.preferential_attachment(graph, [(u, v)]))\n",
    "        features['pref_attachment'] = preferential_scores[0][2] if preferential_scores else 0\n",
    "        '''\n",
    "        # Common Neighbors\n",
    "        common_neighbors = list(nx.common_neighbors(graph, u, v))\n",
    "        features['common_neighbors'] = len(common_neighbors)\n",
    "        '''\n",
    "        # Resource Allocation Index\n",
    "        features['resource_allocation'] = sum(1 / graph.degree(w) for w in common_neighbors if graph.degree(w) > 0)\n",
    "        '''\n",
    "        '''\n",
    "        # Shortest Path Length (if path exists)\n",
    "        try:\n",
    "            features['shortest_path'] = nx.shortest_path_length(graph, u, v)\n",
    "        except nx.NetworkXNoPath:\n",
    "            features['shortest_path'] = num_nodes  # Large value if no path\n",
    "        '''\n",
    "        '''\n",
    "        # Degree Centrality Difference\n",
    "        features['degree_diff'] = abs(graph.degree(u) - graph.degree(v))\n",
    "        '''\n",
    "        # Node Degrees\n",
    "        features['degree_u'] = graph.degree(u)\n",
    "        features['degree_v'] = graph.degree(v)\n",
    "\n",
    "    except nx.NetworkXError:\n",
    "        features['jaccard'] = 0\n",
    "        features['adamic_adar'] = 0\n",
    "        features['common_neighbors'] = 0\n",
    "        features['degree_u'] = 0\n",
    "        features['degree_v'] = 0\n",
    "\n",
    "    return features\n",
    "\n",
    "# --- 5. Prepare Training Data ---\n",
    "X_train_list = []\n",
    "y_train = []\n",
    "\n",
    "# Positive examples\n",
    "print(\"Calculating features for positive training examples...\")\n",
    "for _, row in train_df.iterrows():\n",
    "    u, v = row['node1'], row['node2']\n",
    "    if G.has_node(u) and G.has_node(v):\n",
    "        X_train_list.append(calculate_features(u, v, G))\n",
    "        y_train.append(1)\n",
    "\n",
    "# Negative examples\n",
    "print(\"Calculating features for negative training examples...\")\n",
    "for u, v in negative_samples:\n",
    "    if G.has_node(u) and G.has_node(v):\n",
    "        X_train_list.append(calculate_features(u, v, G))\n",
    "        y_train.append(0)\n",
    "\n",
    "X_train_df = pd.DataFrame(X_train_list)\n",
    "y_train = np.array(y_train)\n",
    "print(\"Training features prepared.\")\n",
    "print(X_train_df.head())\n",
    "print(f\"Training features shape: {X_train_df.shape}\")\n",
    "print(f\"Training labels length: {len(y_train)}\")\n",
    "\n",
    "# --- 6. Split Data into Training and Validation Sets ---\n",
    "print(\"Splitting data into 70% training and 30% validation...\")\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "    X_train_df, y_train, test_size=0.3, random_state=42, stratify=y_train\n",
    ")\n",
    "print(f\"Training split shape: {X_train_split.shape}\")\n",
    "print(f\"Validation split shape: {X_val_split.shape}\")\n",
    "\n",
    "# --- 7. Train Classifier on Split Data ---\n",
    "if len(X_train_split) != len(y_train_split):\n",
    "    print(f\"Error: Mismatch between training features ({len(X_train_split)}) and labels ({len(y_train_split)}).\")\n",
    "else:\n",
    "    print(\"Training XGBClassifier on 70% training split...\")\n",
    "    classifier = XGBClassifier(\n",
    "        n_estimators=400,\n",
    "        objective='binary:logistic',\n",
    "        learning_rate=0.15,\n",
    "        max_depth=5,  # Reduced to prevent overfitting\n",
    "        min_child_weight=7,  # Increased regularization\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    classifier.fit(X_train_split, y_train_split)\n",
    "    \n",
    "    # Calculate training metrics\n",
    "    print(\"Calculating training metrics...\")\n",
    "    train_predictions = classifier.predict(X_train_split)\n",
    "    train_accuracy = accuracy_score(y_train_split, train_predictions)\n",
    "    train_f1 = f1_score(y_train_split, train_predictions)\n",
    "    train_auc = roc_auc_score(y_train_split, classifier.predict_proba(X_train_split)[:, 1])\n",
    "    print(f\"Training accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"Training F1-score: {train_f1:.4f}\")\n",
    "    print(f\"Training AUC: {train_auc:.4f}\")\n",
    "    \n",
    "    # Calculate validation metrics\n",
    "    print(\"Calculating validation metrics...\")\n",
    "    val_predictions = classifier.predict(X_val_split)\n",
    "    val_accuracy = accuracy_score(y_val_split, val_predictions)\n",
    "    val_f1 = f1_score(y_val_split, val_predictions)\n",
    "    val_auc = roc_auc_score(y_val_split, classifier.predict_proba(X_val_split)[:, 1])\n",
    "    print(f\"Validation accuracy: {val_accuracy:.4f}\")\n",
    "    print(f\"Validation F1-score: {val_f1:.4f}\")\n",
    "    print(f\"Validation AUC: {val_f1:.4f}\")\n",
    "    \n",
    "    # Print feature importances\n",
    "    print(\"Feature importances:\", dict(zip(X_train_df.columns, classifier.feature_importances_)))\n",
    "\n",
    "# --- 8. Cross-Validation on Full Training Data ---\n",
    "print(\"Performing 5-fold cross-validation on full training data...\")\n",
    "classifier_cv = XGBClassifier(\n",
    "    n_estimators=400,\n",
    "    objective='binary:logistic',\n",
    "    learning_rate=0.15,\n",
    "    max_depth=5,\n",
    "    min_child_weight=7,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "cv_scores = cross_val_score(classifier_cv, X_train_df, y_train, cv=5, scoring='accuracy')\n",
    "cv_f1_scores = cross_val_score(classifier_cv, X_train_df, y_train, cv=5, scoring='f1')\n",
    "print(f\"Cross-validation accuracy: {np.mean(cv_scores):.4f} ± {np.std(cv_scores):.4f}\")\n",
    "print(f\"Cross-validation F1-score: {np.mean(cv_f1_scores):.4f} ± {np.std(cv_f1_scores):.4f}\")\n",
    "\n",
    "# --- 9. Train Ensemble for Kaggle Submission ---\n",
    "print(\"Training ensemble (XGBoost + RandomForest) on full training data...\")\n",
    "xgb_classifier = XGBClassifier(\n",
    "    n_estimators=400,\n",
    "    objective='binary:logistic',\n",
    "    learning_rate=0.15,\n",
    "    max_depth=5,\n",
    "    min_child_weight=7,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_classifier = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=5,\n",
    "    min_samples_leaf=7,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "xgb_classifier.fit(X_train_df, y_train)\n",
    "rf_classifier.fit(X_train_df, y_train)\n",
    "print(\"Ensemble classifiers trained.\")\n",
    "\n",
    "# --- 10. Prepare Test Data ---\n",
    "print(\"Calculating features for test examples...\")\n",
    "X_test_list = []\n",
    "for _, row in test_df.iterrows():\n",
    "    u, v = row['node1'], row['node2']\n",
    "    if G.has_node(u) and G.has_node(v):\n",
    "        X_test_list.append(calculate_features(u, v, G))\n",
    "    else:\n",
    "        X_test_list.append({\n",
    "            'jaccard': 0,\n",
    "            'adamic_adar': 0,\n",
    "            'common_neighbors': 0,\n",
    "            'degree_u': 0,\n",
    "            'degree_v': 0\n",
    "        })\n",
    "\n",
    "X_test_df = pd.DataFrame(X_test_list)\n",
    "print(\"Test features prepared.\")\n",
    "print(f\"Test features shape: {X_test_df.shape}\")\n",
    "\n",
    "# --- 11. Predict on Test Data (Ensemble) ---\n",
    "print(\"Predicting on test data with ensemble...\")\n",
    "xgb_probs = xgb_classifier.predict_proba(X_test_df)[:, 1]\n",
    "rf_probs = rf_classifier.predict_proba(X_test_df)[:, 1]\n",
    "ensemble_probs = (xgb_probs + rf_probs) / 2\n",
    "predictions = (ensemble_probs > 0.5).astype(int)\n",
    "\n",
    "# --- 12. Format Output ---\n",
    "print(\"Formatting submission file...\")\n",
    "if 'predict_nodepair_id' not in test_df.columns:\n",
    "    test_df['predict_nodepair_id'] = range(len(test_df))\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'predict_nodepair_id': test_df['predict_nodepair_id'],\n",
    "    'ans': predictions\n",
    "})\n",
    "\n",
    "output_filename = 'submission.csv'\n",
    "submission_df.to_csv(output_filename, index=False)\n",
    "print(f\"Submission file '{output_filename}' created successfully.\")\n",
    "print(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b15dcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
